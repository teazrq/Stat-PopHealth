---
title: "TBL Reading (Week 4): Multiple Linear Regression"
author: "Ruoqing Zhu"
date: "Last Updated: `r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    code_folding: hide
    df_print: paged
    toc: yes
    toc_float:
      collapsed: true
      smooth_scroll: true
    toc_depth: '2'
  pdf_document:
    toc: yes
    toc_depth: 2
---

```{r set-options, echo=FALSE, cache=FALSE}
  options(width = 1000)
  knitr::opts_chunk$set(fig.width=9, fig.height=6, out.width = "75%", fig.align = 'center')
  knitr::opts_chunk$set(class.source = "fold-show")
  knitr::opts_chunk$set(collapse=TRUE)
```

## Basic Concept

Multiple linear regression is essentially adding more predictors. A mathematical representation is 

$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p  + \epsilon.$$
And the idea of the optimization is also similar that, we want to minimize the squared errors given a set of observed data. 

$$\underset{\beta_0, \beta_1, \ldots, \beta_p}{\arg\min} \frac{1}{n} \sum_{i=1}^n \big( y_i - \beta_0 - \beta_1 x_{i1} - \beta_2 x_{i2} \cdots - \beta_2 x_{ip}\big)^2$$

It is not straight forward to solve these parameters when given a set of data. This involves applications of some linear algebra:

$$\widehat{\boldsymbol \beta}= (\mathbf{X}^\text{T} \mathbf{X})^{-1} \mathbf{X}^\text{T}\mathbf{y}$$

This can be done using the `lm()` function. For example, if we want to include the longitude in the skin cancer data, we can specify two variables using the `+` sign in the right hand side of `~`:

```{r}
    skincancer = read.table("skin.txt", header = TRUE)
    # fitting a multiple linear regression
    fit = lm(Mort~ Lat + Long, data = skincancer)
    summary(fit)
```

Most of the quantities in this output have similar meanings as a simple linear regression. Except that we have a new variable longitude. However, longitude is not a significant variable. In most cases, you could remove it from the model. 

Predictions can also be made using the `predict()` function by specifying one or more data points. You can also do this by hand by simply plugging in the estimated coefficients to the linear equation. 

```{r}
  testdata = data.frame("Lat" = c(40, 45), "Long" = c(80, 100))
  testdata
  
  # predict this pre-specified testing data
  predict(fit, testdata)
  
  # predict the state of Illinois, which is the 12th row
  predict(fit, skincancer[12, ])
```

## Categorical Predictors

The `Ocean` variable is binary that indicates if a state is next to an ocean. There are also other cases where we may have a categorical variable with many categories. There are two types of categorical variables:

  * Ordinal: the numbers representing each category is ordered, e.g., how many children in a family. Oftentimes nominal data are treated as a continuous variable.
  * Nominal: they are not ordered and can be represented using either numbers or letters, e.g., ethnic group

Nominal data is a more involved topic and we will discuss them later in the logistic regression session. In fact, since `Ocean` is binary and it can be treated as either ordinal or nominal and the model would not change. The following code (`as.factor()`) can be used to specify a nominal categorical variable

```{r}
    fit = lm(Mort~ Lat + Long + as.factor(Ocean), data = skincancer)
    summary(fit)
```

You can notice that in the output, there is a new specification of the parameter estimate, `as.factor(Ocean)1`. This means that the parameter is for estimating the effect of `Ocean`$= 1$. You may wonder where is the effect of category 0. Since this variable has only two categories, we only need one parameter to describe its effect. When`Ocean`$= 1$, we can simply remove the contribution of this parameter. You can validate the result using the following testing data:

```{r}
  testdata = data.frame("Lat" = c(40, 40), 
                        "Long" = c(100, 100),
                        "Ocean" = c(0, 1))
  testdata
  
  # predict and compare the two results
  predict(fit, testdata)
```

## Specifying Higher Order Terms

Sometimes a linear relationship is not sufficient to model the relationship, and we want to include higher order terms and also interactions to explain the effect. For example, we could add the squared term of latitude and the iteration between latitude and longitude to the model. 

```{r}
    fit = lm(Mort~ Lat + Long + as.factor(Ocean) + I(Lat^2) + I(Lat*Long), data = skincancer)
    summary(fit)
```

The prediction can be done using the same dataset we specified before:

```{r}
  predict(fit, testdata)
```

Do you notice the change of significance? What do you think is the cause? Which variable(s) would you like to remove/keep?

## Collinearity

Collinearity refers to the situation that the covariates are (almost) linearly dependent. This is a very severe problem for linear regression. Let's look at an example where we have $Y = X_1 + X_2 + \epsilon$, and variables $X_1$ and $X_2$ are highly correlated with each other. The true parameter should be 1 and 1, respectively. Since we know the true relationship, both variables should be highly significant. However, this is not the case while we fit the regression model. Moreover, the parameter estimates become 6.36779 and -4.34668, which is awfully wrong. However, their difference is almost 2, and this model explained more than 75% of the variation. Can you think of why?

```{r}
  set.seed(1)
  x1 = rnorm(100)
  x2 = x1 + rnorm(100, sd = 0.01)
  y = x1 + x2 + rnorm(100)
  fit = lm(y~ x1 + x2)
  summary(fit)
```

On the other hand, if we create independent covariates with exactly the same model, we see the expected results. 
```{r}
  set.seed(1)
  x1 = rnorm(100)
  x2 = rnorm(100)
  y = x1 + x2 + rnorm(100)
  fit = lm(y~ x1 + x2)
  summary(fit)
```

In practice, you may want to eliminate variables with "almost the same definition", which can be highly correlated. However, this is inevitable if we have a large number of variables, $p$. And eventually, when $p$ is close or larger than the sample size $n$, a linear regression model cannot be used. In that case, we may need to decide which variable to use, instead of using all of them. This concept is called "variable selection" in high-dimensional data, and many machine learning models can be used to address this problem. Although we will not cover that topic in this course, a simple strategy called the **step-wise regression** could be used. It simply adding or subtracting variables from the model one-by-one based on the p-value. 

## Confunding Variables

Let's imaging a relationship with $X$ (e.g. a genetic factor) causes both $Z$ (a symptom) and $Y$ (another symptom), which can be demonstrated using the following code:

```{r}
  set.seed(1)
  X = rnorm(100)
  Z = X + rnorm(100, sd = 0.5)
  Y = X + rnorm(100, sd = 0.5)
```

If we simply regress $Y$ on $Z$, we may conclude that $Z$ would increase $Y$, as $Z$ is highly significant. However, this is not really the true relationship. 

```{r}
  summary(lm(Y~Z))$coefficients
```

In this case, we should also control for the effect of $X$, and in this case, $Z$ is not significant anymore. This means, given the information of $X$, $Z$ does not affect $Y$. 

```{r}
  summary(lm(Y~Z+X))$coefficients
```

In this case, $X$ is a confounding variable. Not including appropriate confounding variables could often lead to wrong conclusions. Commonly used confounding variables are age, gender and race. However, you should always decide that based on the specific study. Here are some comments on confounding variables:

  * Confounding is a causal concept and most statistical methods cannot directly claim causal effect. Whether the result you found is a causal effect should mainly relying on the domain knowledge. 
  * Confounding variables, if included in the model, should stay there even they are not significant.

## Practice Questions 

  1. Fit a multiple linear regression models the Child's height using the `GaltonFamilies` dataset. You should consider the following terms in your regression:
  
      * Father's height
      * Mother's height
      * An interaction between father and mother's heights
      * Number of children in the family, as a nominal categorical variable 
    
```{r class.source = NULL, eval = FALSE}
  library(HistData)
  data(GaltonFamilies)
  summary(lm(childHeight ~ father + mother + I(father*mother) + as.factor(children), data = GaltonFamilies))
```

  2. Do you notice any problem with this model? What could be the causes? Now, instead of defining the number of children as a nominal categorical variable, treat it as an ordinal one (continuous). What is the most significant variable in this new model?
  
```{r class.source = NULL, eval = FALSE}  
  summary(lm(childHeight ~ father + mother + I(father*mother) + children, data = GaltonFamilies))
```

  3. The result still contradict with our understanding that both father and mother's heights would affect a child's height. Remove the interaction term and refit the model. What do you think is the cause of this change? Based on this model, predict a child's height if the father is 75, mother is 65 and there are 3 children in this family. 
  
```{r class.source = NULL, eval = FALSE} 
  summary(lm(childHeight ~ father + mother + children, data = GaltonFamilies))
```

  4. Can you think of examples in which not including a confounding variable would cause a misleading result? 

  5. Read Table 2 in [Goodings et al. (2011)](papers/Gooding2011.pdf) and contrasting Model 2 and Model 3. What could be the cause that the category `Multiethnic` parameter estimate changed quite significantly?

